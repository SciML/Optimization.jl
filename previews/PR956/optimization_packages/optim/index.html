<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optim.jl · Optimization.jl</title><meta name="title" content="Optim.jl · Optimization.jl"/><meta property="og:title" content="Optim.jl · Optimization.jl"/><meta property="twitter:title" content="Optim.jl · Optimization.jl"/><meta name="description" content="Documentation for Optimization.jl."/><meta property="og:description" content="Documentation for Optimization.jl."/><meta property="twitter:description" content="Documentation for Optimization.jl."/><meta property="og:url" content="https://docs.sciml.ai/Optimization/stable/optimization_packages/optim/"/><meta property="twitter:url" content="https://docs.sciml.ai/Optimization/stable/optimization_packages/optim/"/><link rel="canonical" href="https://docs.sciml.ai/Optimization/stable/optimization_packages/optim/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Optimization.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Optimization.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Optimization.jl: A Unified Optimization Package</a></li><li><a class="tocitem" href="../../getting_started/">Getting Started with Optimization.jl</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/certification/">Using SymbolicAnalysis.jl for convexity certificates</a></li><li><a class="tocitem" href="../../tutorials/constraints/">Using Equality and Inequality Constraints</a></li><li><a class="tocitem" href="../../tutorials/ensemble/">Multistart optimization with EnsembleProblem</a></li><li><a class="tocitem" href="../../tutorials/linearandinteger/">Linear and Integer Optimization Problems</a></li><li><a class="tocitem" href="../../tutorials/minibatch/">Data Iterators and Minibatching</a></li><li><a class="tocitem" href="../../tutorials/remakecomposition/">Creating polyalgorithms by chaining solvers using <code>remake</code></a></li><li><a class="tocitem" href="../../tutorials/reusage_interface/">Optimization Problem Reusage and Caching Interface</a></li><li><a class="tocitem" href="../../tutorials/symbolic/">Symbolic Problem Building with ModelingToolkit</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../../examples/rosenbrock/">Solving the Rosenbrock Problem in &gt;10 Ways</a></li></ul></li><li><span class="tocitem">Basics</span><ul><li><a class="tocitem" href="../../API/optimization_problem/">Defining OptimizationProblems</a></li><li><a class="tocitem" href="../../API/optimization_function/">OptimizationFunction</a></li><li><a class="tocitem" href="../../API/ad/">Automatic Differentiation Construction Choice Recommendations</a></li><li><a class="tocitem" href="../../API/solve/">Common Solver Options (Solve Keyword Arguments)</a></li><li><a class="tocitem" href="../../API/optimization_solution/">Optimization Solutions</a></li><li><a class="tocitem" href="../../API/optimization_state/">OptimizationState</a></li><li><a class="tocitem" href="../../API/optimization_stats/">OptimizationStats</a></li><li><a class="tocitem" href="../../API/modelingtoolkit/">ModelingToolkit Integration</a></li><li><a class="tocitem" href="../../API/FAQ/">Frequently Asked Questions</a></li></ul></li><li><span class="tocitem">Optimizer Packages</span><ul><li><a class="tocitem" href="../blackboxoptim/">BlackBoxOptim.jl</a></li><li><a class="tocitem" href="../cmaevolutionstrategy/">CMAEvolutionStrategy.jl</a></li><li><a class="tocitem" href="../evolutionary/">Evolutionary.jl</a></li><li><a class="tocitem" href="../gcmaes/">GCMAES.jl</a></li><li><a class="tocitem" href="../manopt/">Manopt.jl</a></li><li><a class="tocitem" href="../mathoptinterface/">MathOptInterface.jl</a></li><li><a class="tocitem" href="../metaheuristics/">Metaheuristics.jl</a></li><li><a class="tocitem" href="../multistartoptimization/">MultistartOptimization.jl</a></li><li><a class="tocitem" href="../nlopt/">NLopt.jl</a></li><li><a class="tocitem" href="../nlpmodels/">NLPModels.jl</a></li><li><a class="tocitem" href="../nomad/">NOMAD.jl</a></li><li class="is-active"><a class="tocitem" href>Optim.jl</a><ul class="internal"><li><a class="tocitem" href="#Installation:-OptimizationOptimJL.jl"><span>Installation: OptimizationOptimJL.jl</span></a></li><li><a class="tocitem" href="#Methods"><span>Methods</span></a></li><li><a class="tocitem" href="#Local-Optimizer"><span>Local Optimizer</span></a></li><li><a class="tocitem" href="#Global-Optimizer"><span>Global Optimizer</span></a></li></ul></li><li><a class="tocitem" href="../optimisers/">Optimisers.jl</a></li><li><a class="tocitem" href="../optimization/">Optimization.jl</a></li><li><a class="tocitem" href="../polyopt/">Polyalgorithms.jl</a></li><li><a class="tocitem" href="../prima/">PRIMA.jl</a></li><li><a class="tocitem" href="../pycma/">PyCMA.jl</a></li><li><a class="tocitem" href="../quaddirect/">QuadDIRECT.jl</a></li><li><a class="tocitem" href="../speedmapping/">SpeedMapping.jl</a></li><li><a class="tocitem" href="../scipy/">SciPy.jl</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Optimizer Packages</a></li><li class="is-active"><a href>Optim.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optim.jl</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/Optimization.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/Optimization.jl/blob/master/docs/src/optimization_packages/optim.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="optim"><a class="docs-heading-anchor" href="#optim">Optim.jl</a><a id="optim-1"></a><a class="docs-heading-anchor-permalink" href="#optim" title="Permalink"></a></h1><p><a href="https://github.com/JuliaNLSolvers/Optim.jl"><code>Optim</code></a> is Julia package implementing various algorithms to perform univariate and multivariate optimization.</p><h2 id="Installation:-OptimizationOptimJL.jl"><a class="docs-heading-anchor" href="#Installation:-OptimizationOptimJL.jl">Installation: OptimizationOptimJL.jl</a><a id="Installation:-OptimizationOptimJL.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Installation:-OptimizationOptimJL.jl" title="Permalink"></a></h2><p>To use this package, install the OptimizationOptimJL package:</p><pre><code class="language-julia hljs">import Pkg;
Pkg.add(&quot;OptimizationOptimJL&quot;);</code></pre><h2 id="Methods"><a class="docs-heading-anchor" href="#Methods">Methods</a><a id="Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Methods" title="Permalink"></a></h2><p><code>Optim.jl</code> algorithms can be one of the following:</p><ul><li><code>Optim.NelderMead()</code></li><li><code>Optim.SimulatedAnnealing()</code></li><li><code>Optim.ParticleSwarm()</code></li><li><code>Optim.ConjugateGradient()</code></li><li><code>Optim.GradientDescent()</code></li><li><code>Optim.BFGS()</code></li><li><code>Optim.LBFGS()</code></li><li><code>Optim.NGMRES()</code></li><li><code>Optim.OACCEL()</code></li><li><code>Optim.NewtonTrustRegion()</code></li><li><code>Optim.Newton()</code></li><li><code>Optim.KrylovTrustRegion()</code></li><li><code>Optim.ParticleSwarm()</code></li><li><code>Optim.SAMIN()</code></li></ul><p>Each optimizer also takes special arguments which are outlined in the sections below.</p><p>The following special keyword arguments which are not covered by the common <code>solve</code> arguments can be used with Optim.jl optimizers:</p><ul><li><code>x_tol</code>: Absolute tolerance in changes of the input vector <code>x</code>, in infinity norm. Defaults to <code>0.0</code>.</li><li><code>g_tol</code>: Absolute tolerance in the gradient, in infinity norm. Defaults to <code>1e-8</code>. For gradient free methods, this will control the main convergence tolerance, which is solver-specific.</li><li><code>f_calls_limit</code>: A soft upper limit on the number of objective calls. Defaults to <code>0</code> (unlimited).</li><li><code>g_calls_limit</code>: A soft upper limit on the number of gradient calls. Defaults to <code>0</code> (unlimited).</li><li><code>h_calls_limit</code>: A soft upper limit on the number of Hessian calls. Defaults to <code>0</code> (unlimited).</li><li><code>allow_f_increases</code>: Allow steps that increase the objective value. Defaults to <code>false</code>. Note that, when setting this to <code>true</code>, the last iterate will be returned as the minimizer even if the objective increased.</li><li><code>store_trace</code>: Should a trace of the optimization algorithm&#39;s state be stored? Defaults to <code>false</code>.</li><li><code>show_trace</code>: Should a trace of the optimization algorithm&#39;s state be shown on <code>stdout</code>? Defaults to <code>false</code>.</li><li><code>extended_trace</code>: Save additional information. Solver dependent. Defaults to <code>false</code>.</li><li><code>trace_simplex</code>: Include the full simplex in the trace for <code>NelderMead</code>. Defaults to <code>false</code>.</li><li><code>show_every</code>: Trace output is printed every <code>show_every</code>th iteration.</li></ul><p>For a more extensive documentation of all the algorithms and options, please consult the <a href="https://julianlsolvers.github.io/Optim.jl/stable/#"><code>Documentation</code></a></p><h2 id="Local-Optimizer"><a class="docs-heading-anchor" href="#Local-Optimizer">Local Optimizer</a><a id="Local-Optimizer-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Optimizer" title="Permalink"></a></h2><h3 id="Local-Constraint"><a class="docs-heading-anchor" href="#Local-Constraint">Local Constraint</a><a id="Local-Constraint-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Constraint" title="Permalink"></a></h3><p><code>Optim.jl</code> implements the following local constraint algorithms:</p><ul><li><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/ipnewton/"><code>Optim.IPNewton()</code></a><ul><li><p><code>μ0</code> specifies the initial barrier penalty coefficient as either a number or <code>:auto</code></p></li><li><p><code>show_linesearch</code> is an option to turn on linesearch verbosity.</p></li><li><p>Defaults:</p><ul><li><code>linesearch::Function = Optim.backtrack_constrained_grad</code></li><li><code>μ0::Union{Symbol,Number} = :auto</code></li><li><code>show_linesearch::Bool = false</code></li></ul></li></ul></li></ul><p>The Rosenbrock function with constraints can be optimized using the <code>Optim.IPNewton()</code> as follows:</p><pre><code class="language-julia hljs">using Optimization, OptimizationOptimJL
rosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2
cons = (res, x, p) -&gt; res .= [x[1]^2 + x[2]^2]
x0 = zeros(2)
p = [1.0, 100.0]
prob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff(); cons = cons)
prob = Optimization.OptimizationProblem(prob, x0, p, lcons = [-5.0], ucons = [10.0])
sol = solve(prob, IPNewton())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 2-element Vector{Float64}:
 0.9999999992669327
 0.9999999985109471</code></pre><p>See also in the <code>Optim.jl</code> documentation the <a href="https://julianlsolvers.github.io/Optim.jl/stable/#examples/generated/ipnewton_basics/">Nonlinear constrained optimization</a> example using <code>IPNewton</code>.</p><h3 id="Derivative-Free"><a class="docs-heading-anchor" href="#Derivative-Free">Derivative-Free</a><a id="Derivative-Free-1"></a><a class="docs-heading-anchor-permalink" href="#Derivative-Free" title="Permalink"></a></h3><p>Derivative-free optimizers are optimizers that can be used even in cases where no derivatives or automatic differentiation is specified. While they tend to be less efficient than derivative-based optimizers, they can be easily applied to cases where defining derivatives is difficult. Note that while these methods do not support general constraints, all support bounds constraints via <code>lb</code> and <code>ub</code> in the <code>Optimization.OptimizationProblem</code>.</p><p><code>Optim.jl</code> implements the following derivative-free algorithms:</p><ul><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/nelder_mead/"><code>Optim.NelderMead()</code></a>: <strong>Nelder-Mead optimizer</strong></p><ul><li><p><code>solve(problem, NelderMead(parameters, initial_simplex))</code></p></li><li><p><code>parameters = AdaptiveParameters()</code> or <code>parameters = FixedParameters()</code></p></li><li><p><code>initial_simplex = AffineSimplexer()</code></p></li><li><p>Defaults:</p><ul><li><code>parameters = AdaptiveParameters()</code></li><li><code>initial_simplex = AffineSimplexer()</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/simulated_annealing/"><code>Optim.SimulatedAnnealing()</code></a>: <strong>Simulated Annealing</strong></p><ul><li><p><code>solve(problem, SimulatedAnnealing(neighbor, T, p))</code></p></li><li><p><code>neighbor</code> is a mutating function of the current and proposed <code>x</code></p></li><li><p><code>T</code> is a function of the current iteration that returns a temperature</p></li><li><p><code>p</code> is a function of the current temperature</p></li><li><p>Defaults:</p><ul><li><code>neighbor = default_neighbor!</code></li><li><code>T = default_temperature</code></li><li><code>p = kirkpatrick</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/particle_swarm/"><code>Optim.ParticleSwarm()</code></a></p></li></ul><p>The Rosenbrock function can be optimized using the <code>Optim.NelderMead()</code> as follows:</p><pre><code class="language-julia hljs">using Optimization, OptimizationOptimJL
rosenbrock(x, p) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2
x0 = zeros(2)
p = [1.0, 100.0]
prob = Optimization.OptimizationProblem(rosenbrock, x0, p)
sol = solve(prob, Optim.NelderMead())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 2-element Vector{Float64}:
 0.9999634355313174
 0.9999315506115275</code></pre><h3 id="Gradient-Based"><a class="docs-heading-anchor" href="#Gradient-Based">Gradient-Based</a><a id="Gradient-Based-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Based" title="Permalink"></a></h3><p>Gradient-based optimizers are optimizers which utilize the gradient information based on derivatives defined or automatic differentiation.</p><p><code>Optim.jl</code> implements the following gradient-based algorithms:</p><ul><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/cg/"><code>Optim.ConjugateGradient()</code></a>: <strong>Conjugate Gradient Descent</strong></p><ul><li><p><code>solve(problem, ConjugateGradient(alphaguess, linesearch, eta, P, precondprep))</code></p></li><li><p><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)</p><ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><p><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)</p><ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><p><code>eta</code> determines the next step direction</p></li><li><p><code>P</code> is an optional preconditioner (for more information, see <a href="https://julianlsolvers.github.io/Optim.jl/v0.9.3/algo/precondition/">this source</a>)</p></li><li><p><code>precondpred</code> is used to update <code>P</code> as the state variable <code>x</code> changes</p></li><li><p>Defaults:</p><ul><li><code>alphaguess = LineSearches.InitialHagerZhang()</code></li><li><code>linesearch = LineSearches.HagerZhang()</code></li><li><code>eta = 0.4</code></li><li><code>P = nothing</code></li><li><code>precondprep = (P, x) -&gt; nothing</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/gradientdescent/"><code>Optim.GradientDescent()</code></a>: <strong>Gradient Descent (a quasi-Newton solver)</strong></p><ul><li><p><code>solve(problem, GradientDescent(alphaguess, linesearch, P, precondprep))</code></p></li><li><p><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)</p><ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><p><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)</p><ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><p><code>P</code> is an optional preconditioner (for more information, see <a href="https://julianlsolvers.github.io/Optim.jl/v0.9.3/algo/precondition/">this source</a>)</p></li><li><p><code>precondpred</code> is used to update <code>P</code> as the state variable <code>x</code> changes</p></li><li><p>Defaults:</p><ul><li><code>alphaguess = LineSearches.InitialPrevious()</code></li><li><code>linesearch = LineSearches.HagerZhang()</code></li><li><code>P = nothing</code></li><li><code>precondprep = (P, x) -&gt; nothing</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/lbfgs/"><code>Optim.BFGS()</code></a>: <strong>Broyden-Fletcher-Goldfarb-Shanno algorithm</strong></p><ul><li><p><code>solve(problem, BFGS(alphaguess, linesearch, initial_invH, initial_stepnorm, manifold))</code></p></li><li><p><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)</p><ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><p><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)</p><ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><p><code>initial_invH</code> specifies an optional initial matrix</p></li><li><p><code>initial_stepnorm</code> determines that <code>initial_invH</code> is an identity matrix scaled by the value of <code>initial_stepnorm</code> multiplied by the sup-norm of the gradient at the initial point</p></li><li><p><code>manifold</code> specifies a (Riemannian) manifold on which the function is to be minimized (for more information, consult <a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/manifolds/">this source</a>)</p><ul><li>available manifolds:</li><li><code>Flat</code></li><li><code>Sphere</code></li><li><code>Stiefel</code></li><li>meta-manifolds:</li><li><code>PowerManifold</code></li><li><code>ProductManifold</code></li><li>custom manifolds</li></ul></li><li><p>Defaults:</p><ul><li><code>alphaguess = LineSearches.InitialStatic()</code></li><li><code>linesearch = LineSearches.HagerZhang()</code></li><li><code>initial_invH = nothing</code></li><li><code>initial_stepnorm = nothing</code></li><li><code>manifold = Flat()</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/lbfgs/"><code>Optim.LBFGS()</code></a>: <strong>Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm</strong></p><ul><li><p><code>m</code> is the number of history points</p></li><li><p><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)</p><ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><p><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)</p><ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><p><code>P</code> is an optional preconditioner (for more information, see <a href="https://julianlsolvers.github.io/Optim.jl/v0.9.3/algo/precondition/">this source</a>)</p></li><li><p><code>precondpred</code> is used to update <code>P</code> as the state variable <code>x</code> changes</p></li><li><p><code>manifold</code> specifies a (Riemannian) manifold on which the function is to be minimized (for more information, consult <a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/manifolds/">this source</a>)</p><ul><li>available manifolds:</li><li><code>Flat</code></li><li><code>Sphere</code></li><li><code>Stiefel</code></li><li>meta-manifolds:</li><li><code>PowerManifold</code></li><li><code>ProductManifold</code></li><li>custom manifolds</li></ul></li><li><p><code>scaleinvH0</code>: whether to scale the initial Hessian approximation</p></li><li><p>Defaults:</p><ul><li><code>m = 10</code></li><li><code>alphaguess = LineSearches.InitialStatic()</code></li><li><code>linesearch = LineSearches.HagerZhang()</code></li><li><code>P = nothing</code></li><li><code>precondprep = (P, x) -&gt; nothing</code></li><li><code>manifold = Flat()</code></li><li><code>scaleinvH0::Bool = true &amp;&amp; (P isa Nothing)</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/ngmres/"><code>Optim.NGMRES()</code></a></p></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/ngmres/"><code>Optim.OACCEL()</code></a></p></li></ul><p>The Rosenbrock function can be optimized using the <code>Optim.LBFGS()</code> as follows:</p><pre><code class="language-julia hljs">using Optimization, OptimizationOptimJL
rosenbrock(x, p) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2
x0 = zeros(2)
p = [1.0, 100.0]
optprob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())
prob = Optimization.OptimizationProblem(optprob, x0, p, lb = [-1.0, -1.0], ub = [0.8, 0.8])
sol = solve(prob, Optim.LBFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 2-element Vector{Float64}:
 0.799999998888889
 0.6399999982096882</code></pre><h3 id="Hessian-Based-Second-Order"><a class="docs-heading-anchor" href="#Hessian-Based-Second-Order">Hessian-Based Second Order</a><a id="Hessian-Based-Second-Order-1"></a><a class="docs-heading-anchor-permalink" href="#Hessian-Based-Second-Order" title="Permalink"></a></h3><p>Hessian-based optimization methods are second order optimization methods which use the direct computation of the Hessian. These can converge faster, but require fast and accurate methods for calculating the Hessian in order to be appropriate.</p><p><code>Optim.jl</code> implements the following hessian-based algorithms:</p><ul><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/newton_trust_region/"><code>Optim.NewtonTrustRegion()</code></a>: <strong>Newton Trust Region method</strong></p><ul><li><p><code>initial_delta</code>: The starting trust region radius</p></li><li><p><code>delta_hat</code>: The largest allowable trust region radius</p></li><li><p><code>eta</code>: When rho is at least eta, accept the step.</p></li><li><p><code>rho_lower</code>: When rho is less than rho_lower, shrink the trust region.</p></li><li><p><code>rho_upper</code>: When rho is greater than rho<em>upper, grow the trust region (though no greater than delta</em>hat).</p></li><li><p>Defaults:</p><ul><li><code>initial_delta = 1.0</code></li><li><code>delta_hat = 100.0</code></li><li><code>eta = 0.1</code></li><li><code>rho_lower = 0.25</code></li><li><code>rho_upper = 0.75</code></li></ul></li></ul></li><li><p><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/newton/"><code>Optim.Newton()</code></a>: <strong>Newton&#39;s method with line search</strong></p><ul><li><p><code>alphaguess</code> computes the initial step length (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_initialstep.html">this example</a>)</p><ul><li>available initial step length procedures:</li><li><code>InitialPrevious</code></li><li><code>InitialStatic</code></li><li><code>InitialHagerZhang</code></li><li><code>InitialQuadratic</code></li><li><code>InitialConstantChange</code></li></ul></li><li><p><code>linesearch</code> specifies the line search algorithm (for more information, consult <a href="https://github.com/JuliaNLSolvers/LineSearches.jl">this source</a> and <a href="https://julianlsolvers.github.io/LineSearches.jl/latest/examples/generated/optim_linesearch.html">this example</a>)</p><ul><li>available line search algorithms:</li><li><code>HaegerZhang</code></li><li><code>MoreThuente</code></li><li><code>BackTracking</code></li><li><code>StrongWolfe</code></li><li><code>Static</code></li></ul></li><li><p>Defaults:</p><ul><li><code>alphaguess = LineSearches.InitialStatic()</code></li><li><code>linesearch = LineSearches.HagerZhang()</code></li></ul></li></ul></li></ul><p>The Rosenbrock function can be optimized using the <code>Optim.Newton()</code> as follows:</p><pre><code class="language-julia hljs">using Optimization, OptimizationOptimJL, ModelingToolkit
rosenbrock(x, p) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2
x0 = zeros(2)
p = [1.0, 100.0]
f = OptimizationFunction(rosenbrock, Optimization.AutoModelingToolkit())
prob = Optimization.OptimizationProblem(f, x0, p)
sol = solve(prob, Optim.Newton())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 2-element Vector{Float64}:
 0.9999999999999994
 0.9999999999999989</code></pre><h3 id="Hessian-Free-Second-Order"><a class="docs-heading-anchor" href="#Hessian-Free-Second-Order">Hessian-Free Second Order</a><a id="Hessian-Free-Second-Order-1"></a><a class="docs-heading-anchor-permalink" href="#Hessian-Free-Second-Order" title="Permalink"></a></h3><p>Hessian-free methods are methods which perform second order optimization by direct computation of Hessian-vector products (<code>Hv</code>) without requiring the construction of the full Hessian. As such, these methods can perform well for large second order optimization problems, but can require special case when considering conditioning of the Hessian.</p><p><code>Optim.jl</code> implements the following hessian-free algorithms:</p><ul><li><code>Optim.KrylovTrustRegion()</code>: <strong>A Newton-Krylov method with Trust Regions</strong><ul><li><p><code>initial_delta</code>: The starting trust region radius</p></li><li><p><code>delta_hat</code>: The largest allowable trust region radius</p></li><li><p><code>eta</code>: When rho is at least eta, accept the step.</p></li><li><p><code>rho_lower</code>: When rho is less than rho_lower, shrink the trust region.</p></li><li><p><code>rho_upper</code>: When rho is greater than rho<em>upper, grow the trust region (though no greater than delta</em>hat).</p></li><li><p>Defaults:</p><ul><li><code>initial_delta = 1.0</code></li><li><code>delta_hat = 100.0</code></li><li><code>eta = 0.1</code></li><li><code>rho_lower = 0.25</code></li><li><code>rho_upper = 0.75</code></li></ul></li></ul></li></ul><p>The Rosenbrock function can be optimized using the <code>Optim.KrylovTrustRegion()</code> as follows:</p><pre><code class="language-julia hljs">using Optimization, OptimizationOptimJL
rosenbrock(x, p) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2
x0 = zeros(2)
p = [1.0, 100.0]
optprob = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())
prob = Optimization.OptimizationProblem(optprob, x0, p)
sol = solve(prob, Optim.KrylovTrustRegion())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 2-element Vector{Float64}:
 0.999999999999108
 0.9999999999981819</code></pre><h2 id="Global-Optimizer"><a class="docs-heading-anchor" href="#Global-Optimizer">Global Optimizer</a><a id="Global-Optimizer-1"></a><a class="docs-heading-anchor-permalink" href="#Global-Optimizer" title="Permalink"></a></h2><h3 id="Without-Constraint-Equations"><a class="docs-heading-anchor" href="#Without-Constraint-Equations">Without Constraint Equations</a><a id="Without-Constraint-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#Without-Constraint-Equations" title="Permalink"></a></h3><p>The following method in <a href="https://github.com/JuliaNLSolvers/Optim.jl"><code>Optim</code></a> performs global optimization on problems with or without box constraints. It works both with and without lower and upper bounds set by <code>lb</code> and <code>ub</code> in the <code>Optimization.OptimizationProblem</code>.</p><ul><li><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/particle_swarm/"><code>Optim.ParticleSwarm()</code></a>: <strong>Particle Swarm Optimization</strong><ul><li><code>solve(problem, ParticleSwarm(lower, upper, n_particles))</code></li><li><code>lower</code>/<code>upper</code> are vectors of lower/upper bounds respectively</li><li><code>n_particles</code> is the number of particles in the swarm</li><li>defaults to: <code>lower = []</code>, <code>upper = []</code>, <code>n_particles = 0</code></li></ul></li></ul><p>The Rosenbrock function can be optimized using the <code>Optim.ParticleSwarm()</code> as follows:</p><pre><code class="language-julia hljs">using Optimization, OptimizationOptimJL
rosenbrock(x, p) = (p[1] - x[1])^2 + p[2] * (x[2] - x[1]^2)^2
x0 = zeros(2)
p = [1.0, 100.0]
f = OptimizationFunction(rosenbrock)
prob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])
sol = solve(prob, Optim.ParticleSwarm(lower = prob.lb, upper = prob.ub, n_particles = 100))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Failure
u: 2-element Vector{Float64}:
 1.0
 1.0</code></pre><h3 id="With-Constraint-Equations"><a class="docs-heading-anchor" href="#With-Constraint-Equations">With Constraint Equations</a><a id="With-Constraint-Equations-1"></a><a class="docs-heading-anchor-permalink" href="#With-Constraint-Equations" title="Permalink"></a></h3><p>The following method in <a href="https://github.com/JuliaNLSolvers/Optim.jl"><code>Optim</code></a> performs global optimization on problems with box constraints.</p><ul><li><a href="https://julianlsolvers.github.io/Optim.jl/stable/algo/samin/"><code>Optim.SAMIN()</code></a>: <strong>Simulated Annealing with bounds</strong><ul><li><p><code>solve(problem, SAMIN(nt, ns, rt, neps, f_tol, x_tol, coverage_ok, verbosity))</code></p></li><li><p>Defaults:</p><ul><li><code>nt = 5</code></li><li><code>ns = 5</code></li><li><code>rt = 0.9</code></li><li><code>neps = 5</code></li><li><code>f_tol = 1e-12</code></li><li><code>x_tol = 1e-6</code></li><li><code>coverage_ok = false</code></li><li><code>verbosity = 0</code></li></ul></li></ul></li></ul><p>The Rosenbrock function can be optimized using the <code>Optim.SAMIN()</code> as follows:</p><pre><code class="language-julia hljs">using Optimization, OptimizationOptimJL
rosenbrock(x, p) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2
x0 = zeros(2)
p = [1.0, 100.0]
f = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())
prob = Optimization.OptimizationProblem(f, x0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])
sol = solve(prob, Optim.SAMIN())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Failure
u: 2-element Vector{Float64}:
 0.8995658029114213
 0.827041517330908</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../nomad/">« NOMAD.jl</a><a class="docs-footer-nextpage" href="../optimisers/">Optimisers.jl »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Sunday 27 July 2025 19:59">Sunday 27 July 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
