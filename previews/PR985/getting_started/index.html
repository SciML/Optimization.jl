<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Getting Started with Optimization.jl · Optimization.jl</title><meta name="title" content="Getting Started with Optimization.jl · Optimization.jl"/><meta property="og:title" content="Getting Started with Optimization.jl · Optimization.jl"/><meta property="twitter:title" content="Getting Started with Optimization.jl · Optimization.jl"/><meta name="description" content="Documentation for Optimization.jl."/><meta property="og:description" content="Documentation for Optimization.jl."/><meta property="twitter:description" content="Documentation for Optimization.jl."/><meta property="og:url" content="https://docs.sciml.ai/Optimization/stable/getting_started/"/><meta property="twitter:url" content="https://docs.sciml.ai/Optimization/stable/getting_started/"/><link rel="canonical" href="https://docs.sciml.ai/Optimization/stable/getting_started/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="Optimization.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Optimization.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Optimization.jl: A Unified Optimization Package</a></li><li class="is-active"><a class="tocitem" href>Getting Started with Optimization.jl</a><ul class="internal"><li><a class="tocitem" href="#Understanding-the-Solution-Object"><span>Understanding the Solution Object</span></a></li><li><a class="tocitem" href="#Import-a-different-solver-package-and-solve-the-problem"><span>Import a different solver package and solve the problem</span></a></li><li><a class="tocitem" href="#Defining-the-objective-function"><span>Defining the objective function</span></a></li><li><a class="tocitem" href="#Controlling-Gradient-Calculations-(Automatic-Differentiation)"><span>Controlling Gradient Calculations (Automatic Differentiation)</span></a></li><li><a class="tocitem" href="#Setting-Box-Constraints"><span>Setting Box Constraints</span></a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/certification/">Using SymbolicAnalysis.jl for convexity certificates</a></li><li><a class="tocitem" href="../tutorials/constraints/">Using Equality and Inequality Constraints</a></li><li><a class="tocitem" href="../tutorials/ensemble/">Multistart optimization with EnsembleProblem</a></li><li><a class="tocitem" href="../tutorials/linearandinteger/">Linear and Integer Optimization Problems</a></li><li><a class="tocitem" href="../tutorials/minibatch/">Data Iterators and Minibatching</a></li><li><a class="tocitem" href="../tutorials/remakecomposition/">Creating polyalgorithms by chaining solvers using <code>remake</code></a></li><li><a class="tocitem" href="../tutorials/reusage_interface/">Optimization Problem Reusage and Caching Interface</a></li><li><a class="tocitem" href="../tutorials/symbolic/">Symbolic Problem Building with ModelingToolkit</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/rosenbrock/">Solving the Rosenbrock Problem in &gt;10 Ways</a></li></ul></li><li><span class="tocitem">Basics</span><ul><li><a class="tocitem" href="../API/optimization_problem/">Defining OptimizationProblems</a></li><li><a class="tocitem" href="../API/optimization_function/">OptimizationFunction</a></li><li><a class="tocitem" href="../API/ad/">Automatic Differentiation Construction Choice Recommendations</a></li><li><a class="tocitem" href="../API/solve/">Common Solver Options (Solve Keyword Arguments)</a></li><li><a class="tocitem" href="../API/optimization_solution/">Optimization Solutions</a></li><li><a class="tocitem" href="../API/optimization_state/">OptimizationState</a></li><li><a class="tocitem" href="../API/optimization_stats/">OptimizationStats</a></li><li><a class="tocitem" href="../API/modelingtoolkit/">ModelingToolkit Integration</a></li><li><a class="tocitem" href="../API/FAQ/">Frequently Asked Questions</a></li></ul></li><li><span class="tocitem">Optimizer Packages</span><ul><li><a class="tocitem" href="../optimization_packages/blackboxoptim/">BlackBoxOptim.jl</a></li><li><a class="tocitem" href="../optimization_packages/cmaevolutionstrategy/">CMAEvolutionStrategy.jl</a></li><li><a class="tocitem" href="../optimization_packages/evolutionary/">Evolutionary.jl</a></li><li><a class="tocitem" href="../optimization_packages/gcmaes/">GCMAES.jl</a></li><li><a class="tocitem" href="../optimization_packages/manopt/">Manopt.jl</a></li><li><a class="tocitem" href="../optimization_packages/mathoptinterface/">MathOptInterface.jl</a></li><li><a class="tocitem" href="../optimization_packages/metaheuristics/">Metaheuristics.jl</a></li><li><a class="tocitem" href="../optimization_packages/multistartoptimization/">MultistartOptimization.jl</a></li><li><a class="tocitem" href="../optimization_packages/nlopt/">NLopt.jl</a></li><li><a class="tocitem" href="../optimization_packages/nlpmodels/">NLPModels.jl</a></li><li><a class="tocitem" href="../optimization_packages/nomad/">NOMAD.jl</a></li><li><a class="tocitem" href="../optimization_packages/optim/">Optim.jl</a></li><li><a class="tocitem" href="../optimization_packages/optimisers/">Optimisers.jl</a></li><li><a class="tocitem" href="../optimization_packages/optimization/">Optimization.jl</a></li><li><a class="tocitem" href="../optimization_packages/polyopt/">Polyalgorithms.jl</a></li><li><a class="tocitem" href="../optimization_packages/prima/">PRIMA.jl</a></li><li><a class="tocitem" href="../optimization_packages/pycma/">PyCMA.jl</a></li><li><a class="tocitem" href="../optimization_packages/quaddirect/">QuadDIRECT.jl</a></li><li><a class="tocitem" href="../optimization_packages/speedmapping/">SpeedMapping.jl</a></li><li><a class="tocitem" href="../optimization_packages/scipy/">SciPy.jl</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Getting Started with Optimization.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Getting Started with Optimization.jl</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/Optimization.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/Optimization.jl/blob/master/docs/src/getting_started.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Getting-Started-with-Optimization.jl"><a class="docs-heading-anchor" href="#Getting-Started-with-Optimization.jl">Getting Started with Optimization.jl</a><a id="Getting-Started-with-Optimization.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-Started-with-Optimization.jl" title="Permalink"></a></h1><p>In this tutorial, we introduce the basics of Optimization.jl by showing how to easily mix local optimizers and global optimizers on the Rosenbrock equation.</p><p>The Rosenbrock equation is defined as follows:</p><p class="math-container">\[f(u,p) = (p_1 - u_1)^2 + p_2 * ( u_2 - u_1^2)^2\]</p><p>This is a parameterized optimization problem where we want to solve for the vector <code>u</code> s.t. <code>u</code> minimizes <code>f</code>. The simplest copy-pasteable code using a quasi-Newton method (LBFGS) to solve the Rosenbrock problem is the following:</p><pre><code class="language-julia hljs"># Import the package and define the problem to optimize
using Optimization, Zygote
rosenbrock(u, p) = (p[1] - u[1])^2 + p[2] * (u[2] - u[1]^2)^2
u0 = zeros(2)
p = [1.0, 100.0]

optf = OptimizationFunction(rosenbrock, AutoZygote())
prob = OptimizationProblem(optf, u0, p)

sol = solve(prob, Optimization.LBFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 2-element Vector{Float64}:
 0.9999997057368228
 0.999999398151528</code></pre><pre><code class="language-julia hljs">sol.u</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 0.9999997057368228
 0.999999398151528</code></pre><pre><code class="language-julia hljs">sol.objective</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.0433892998247468e-13</code></pre><p>Tada! That&#39;s how you do it. Now let&#39;s dive in a little more into what each part means and how to customize it all to your needs.</p><h2 id="Understanding-the-Solution-Object"><a class="docs-heading-anchor" href="#Understanding-the-Solution-Object">Understanding the Solution Object</a><a id="Understanding-the-Solution-Object-1"></a><a class="docs-heading-anchor-permalink" href="#Understanding-the-Solution-Object" title="Permalink"></a></h2><p>The solution object is a <code>SciMLBase.AbstractNoTimeSolution</code>, and thus it follows the <a href="https://docs.sciml.ai/SciMLBase/stable/interfaces/Solutions/">SciMLBase Solution Interface for non-timeseries objects</a> and is documented at the <a href="../API/optimization_solution/#solution">solution type page</a>. However, for simplicity let&#39;s show a bit of it in action.</p><p>An optimization solution has an array interface so that it acts like the array that it solves for. This array syntax is shorthand for simply grabbing the solution <code>u</code>. For example:</p><pre><code class="language-julia hljs">sol[1] == sol.u[1]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><pre><code class="language-julia hljs">Array(sol) == sol.u</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">true</code></pre><p><code>sol.objective</code> returns the final cost of the optimization. We can validate this by plugging it into our function:</p><pre><code class="language-julia hljs">rosenbrock(sol.u, p)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.0433892998247468e-13</code></pre><pre><code class="language-julia hljs">sol.objective</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">1.0433892998247468e-13</code></pre><p>The <code>sol.retcode</code> gives us more information about the solution process.</p><pre><code class="language-julia hljs">sol.retcode</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ReturnCode.Success = 1</code></pre><p>Here it says <code>ReturnCode.Success</code> which means that the solutuion successfully solved. We can learn more about the different return codes at <a href="https://docs.sciml.ai/SciMLBase/stable/interfaces/Solutions/#retcodes">the ReturnCode part of the SciMLBase documentation</a>.</p><p>If we are interested about some of the statistics of the solving process, for example to help choose a better solver, we can investigate the <code>sol.stats</code></p><pre><code class="language-julia hljs">sol.stats</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">SciMLBase.OptimizationStats
Number of iterations:                              21
Time in seconds:                                   0.247909
Number of function evaluations:                    25
Number of gradient evaluations:                    25
Number of hessian evaluations:                     0</code></pre><p>That&#39;s just a bit of what&#39;s in there, check out the other pages for more information but now let&#39;s move onto customization.</p><h2 id="Import-a-different-solver-package-and-solve-the-problem"><a class="docs-heading-anchor" href="#Import-a-different-solver-package-and-solve-the-problem">Import a different solver package and solve the problem</a><a id="Import-a-different-solver-package-and-solve-the-problem-1"></a><a class="docs-heading-anchor-permalink" href="#Import-a-different-solver-package-and-solve-the-problem" title="Permalink"></a></h2><p>OptimizationOptimJL is a wrapper for <a href="https://github.com/JuliaNLSolvers/Optim.jl">Optim.jl</a> and OptimizationBBO is a wrapper for <a href="https://github.com/robertfeldt/BlackBoxOptim.jl">BlackBoxOptim.jl</a>.</p><p>First let&#39;s use the NelderMead a derivative free solver from Optim.jl:</p><pre><code class="language-julia hljs">using OptimizationOptimJL
sol = solve(prob, Optim.NelderMead())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 2-element Vector{Float64}:
 0.9999634355313174
 0.9999315506115275</code></pre><p>BlackBoxOptim.jl offers derivative-free global optimization solvers that requrie the bounds to be set via <code>lb</code> and <code>ub</code> in the <code>OptimizationProblem</code>. Let&#39;s use the BBO<em>adaptive</em>de<em>rand</em>1<em>bin</em>radiuslimited() solver:</p><pre><code class="language-julia hljs">using OptimizationBBO
prob = OptimizationProblem(rosenbrock, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])
sol = solve(prob, BBO_adaptive_de_rand_1_bin_radiuslimited())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: MaxIters
u: 2-element Vector{Float64}:
 1.0
 1.0</code></pre><p>The solution from the original solver can always be obtained via <code>original</code>:</p><pre><code class="language-julia hljs">sol.original</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">BlackBoxOptim.OptimizationResults(&quot;adaptive_de_rand_1_bin_radiuslimited&quot;, &quot;Max number of steps (10000) reached&quot;, 10001, 1.754068257471031e9, 0.01603412628173828, BlackBoxOptim.ParamsDictChain[BlackBoxOptim.ParamsDictChain[Dict{Symbol, Any}(:RngSeed =&gt; 867298, :SearchRange =&gt; [(-1.0, 1.0), (-1.0, 1.0)], :TraceMode =&gt; :silent, :Method =&gt; :adaptive_de_rand_1_bin_radiuslimited, :MaxSteps =&gt; 10000),Dict{Symbol, Any}()],Dict{Symbol, Any}(:CallbackInterval =&gt; -1.0, :TargetFitness =&gt; nothing, :TraceMode =&gt; :compact, :FitnessScheme =&gt; BlackBoxOptim.ScalarFitnessScheme{true}(), :MinDeltaFitnessTolerance =&gt; 1.0e-50, :NumDimensions =&gt; :NotSpecified, :FitnessTolerance =&gt; 1.0e-8, :TraceInterval =&gt; 0.5, :MaxStepsWithoutProgress =&gt; 10000, :MaxSteps =&gt; 10000…)], 10156, BlackBoxOptim.ScalarFitnessScheme{true}(), BlackBoxOptim.TopListArchiveOutput{Float64, Vector{Float64}}(0.0, [1.0, 1.0]), BlackBoxOptim.PopulationOptimizerOutput{BlackBoxOptim.FitPopulation{Float64}}(BlackBoxOptim.FitPopulation{Float64}([0.9999999999999989 0.9999999999999996 … 1.0 0.999999999999996; 0.999999999999998 0.9999999999999988 … 1.0 0.9999999999999923], NaN, [6.162975822039155e-30, 1.1290571705975731e-29, 1.633188592840376e-29, 4.942706609275402e-30, 4.8823094462194184e-29, 1.1617209424543807e-28, 5.7192415628523356e-30, 1.1142660286246792e-29, 3.2343297114061484e-29, 7.415785547143274e-28  …  1.9855752243929308e-27, 6.5957276583111e-27, 2.7733391199176196e-30, 4.930380657631324e-30, 1.3435287292045357e-30, 2.0078975228203566e-29, 5.534352288191161e-30, 3.315680992257065e-30, 0.0, 2.706778981039597e-29], 0, BlackBoxOptim.Candidate{Float64}[BlackBoxOptim.Candidate{Float64}([0.9999999999999983, 0.9999999999999967], 43, 2.7733391199176196e-30, BlackBoxOptim.AdaptiveDiffEvoRandBin{3}(BlackBoxOptim.AdaptiveDiffEvoParameters(BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.65, σ=0.1), Distributions.Cauchy{Float64}(μ=1.0, σ=0.1), 0.5, false, true), BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.1, σ=0.1), Distributions.Cauchy{Float64}(μ=0.95, σ=0.1), 0.5, false, true), [0.7921066104603902, 0.9610583979898928, 0.8480512091458681, 0.7015401840970414, 0.7357882145575061, 0.9072408188893557, 0.9437891968161449, 0.9450987365508835, 0.5403732766289017, 1.0  …  0.9210909802019588, 1.0, 0.7038257262616895, 1.0, 0.7740409688956934, 0.9457002208071215, 0.7592388091926038, 0.5448494845764422, 0.6498779190318992, 0.8102224158421903], [0.1577721429029223, 0.8697914395285841, 0.9788209484783775, 0.09019743637465322, 0.21686411939320094, 1.0, 0.18986543086330315, 0.14943547813675293, 0.7313858653814128, 0.9590282268909867  …  0.0399329764912994, 0.10557405702704112, 0.1171091809537606, 0.24897154007171143, 0.0741048753498019, 1.0, 0.80995988781013, 0.9169465831287205, 1.0, 0.20006862167761308])), 0), BlackBoxOptim.Candidate{Float64}([0.999999999999994, 0.9999999999999799], 43, 6.604442106123463e-27, BlackBoxOptim.AdaptiveDiffEvoRandBin{3}(BlackBoxOptim.AdaptiveDiffEvoParameters(BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.65, σ=0.1), Distributions.Cauchy{Float64}(μ=1.0, σ=0.1), 0.5, false, true), BlackBoxOptim.BimodalCauchy(Distributions.Cauchy{Float64}(μ=0.1, σ=0.1), Distributions.Cauchy{Float64}(μ=0.95, σ=0.1), 0.5, false, true), [0.7921066104603902, 0.9610583979898928, 0.8480512091458681, 0.7015401840970414, 0.7357882145575061, 0.9072408188893557, 0.9437891968161449, 0.9450987365508835, 0.5403732766289017, 1.0  …  0.9210909802019588, 1.0, 0.7038257262616895, 1.0, 0.7740409688956934, 0.9457002208071215, 0.7592388091926038, 0.5448494845764422, 0.6498779190318992, 0.8102224158421903], [0.1577721429029223, 0.8697914395285841, 0.9788209484783775, 0.09019743637465322, 0.21686411939320094, 1.0, 0.18986543086330315, 0.14943547813675293, 0.7313858653814128, 0.9590282268909867  …  0.0399329764912994, 0.10557405702704112, 0.1171091809537606, 0.24897154007171143, 0.0741048753498019, 1.0, 0.80995988781013, 0.9169465831287205, 1.0, 0.20006862167761308])), 0)], Base.Threads.SpinLock(0))))</code></pre><h2 id="Defining-the-objective-function"><a class="docs-heading-anchor" href="#Defining-the-objective-function">Defining the objective function</a><a id="Defining-the-objective-function-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-the-objective-function" title="Permalink"></a></h2><p>Optimization.jl assumes that your objective function takes two arguments <code>objective(x, p)</code></p><ol><li>The optimization variables <code>x</code>.</li><li>Other parameters <code>p</code>, such as hyper parameters of the cost function. If you have no “other parameters”, you can  safely disregard this argument. If your objective function is defined by someone else, you can create an anonymous function that just discards the extra parameters like this</li></ol><pre><code class="language-julia hljs">obj = (x, p) -&gt; objective(x) # Pass this function into OptimizationFunction</code></pre><h2 id="Controlling-Gradient-Calculations-(Automatic-Differentiation)"><a class="docs-heading-anchor" href="#Controlling-Gradient-Calculations-(Automatic-Differentiation)">Controlling Gradient Calculations (Automatic Differentiation)</a><a id="Controlling-Gradient-Calculations-(Automatic-Differentiation)-1"></a><a class="docs-heading-anchor-permalink" href="#Controlling-Gradient-Calculations-(Automatic-Differentiation)" title="Permalink"></a></h2><p>Notice that both of the above methods were derivative-free methods, and thus no gradients were required to do the optimization. However, often first order optimization (i.e., using gradients) is much more efficient. Defining gradients can be done in two ways. One way is to manually provide a gradient definition in the <code>OptimizationFunction</code> constructor. However, the more convenient way to obtain gradients is to provide an AD backend type.</p><p>For example, let&#39;s now use the OptimizationOptimJL <code>BFGS</code> method to solve the same problem. We will import the forward-mode automatic differentiation library (<code>using ForwardDiff</code>) and then specify in the <code>OptimizationFunction</code> to automatically construct the derivative functions using ForwardDiff.jl. This looks like:</p><pre><code class="language-julia hljs">using ForwardDiff
optf = OptimizationFunction(rosenbrock, Optimization.AutoForwardDiff())
prob = OptimizationProblem(optf, u0, p)
sol = solve(prob, BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 2-element Vector{Float64}:
 0.9999999999373614
 0.999999999868622</code></pre><p>We can inspect the <code>original</code> to see the statistics on the number of steps required and gradients computed:</p><pre><code class="language-julia hljs">sol.original</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"> * Status: success

 * Candidate solution
    Final objective value:     7.645684e-21

 * Found with
    Algorithm:     BFGS

 * Convergence measures
    |x - x&#39;|               = 3.48e-07 ≰ 0.0e+00
    |x - x&#39;|/|x&#39;|          = 3.48e-07 ≰ 0.0e+00
    |f(x) - f(x&#39;)|         = 6.91e-14 ≰ 0.0e+00
    |f(x) - f(x&#39;)|/|f(x&#39;)| = 9.03e+06 ≰ 0.0e+00
    |g(x)|                 = 2.32e-09 ≤ 1.0e-08

 * Work counters
    Seconds run:   0  (vs limit Inf)
    Iterations:    16
    f(x) calls:    53
    ∇f(x) calls:   53
</code></pre><p>Sure enough, it&#39;s a lot less than the derivative-free methods!</p><p>However, the compute cost of forward-mode automatic differentiation scales via the number of inputs, and thus as our optimization problem grows large it slows down. To counteract this, for larger optimization problems (&gt;100 state variables) one normally would want to use reverse-mode automatic differentiation. One common choice for reverse-mode automatic differentiation is Zygote.jl. We can demonstrate this via:</p><pre><code class="language-julia hljs">using Zygote
optf = OptimizationFunction(rosenbrock, Optimization.AutoZygote())
prob = OptimizationProblem(optf, u0, p)
sol = solve(prob, BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 2-element Vector{Float64}:
 0.9999999999373614
 0.999999999868622</code></pre><h2 id="Setting-Box-Constraints"><a class="docs-heading-anchor" href="#Setting-Box-Constraints">Setting Box Constraints</a><a id="Setting-Box-Constraints-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-Box-Constraints" title="Permalink"></a></h2><p>In many cases, one knows the potential bounds on the solution values. In Optimization.jl, these can be supplied as the <code>lb</code> and <code>ub</code> arguments for the lower bounds and upper bounds respectively, supplying a vector of values with one per state variable. Let&#39;s now do our gradient-based optimization with box constraints by rebuilding the OptimizationProblem:</p><pre><code class="language-julia hljs">prob = OptimizationProblem(optf, u0, p, lb = [-1.0, -1.0], ub = [1.0, 1.0])
sol = solve(prob, BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 2-element Vector{Float64}:
 0.9999999993561103
 0.9999999987161009</code></pre><p>For more information on handling constraints, in particular equality and inequality constraints, take a look at the <a href="../tutorials/constraints/#constraints">constraints tutorial</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Optimization.jl: A Unified Optimization Package</a><a class="docs-footer-nextpage" href="../tutorials/certification/">Using SymbolicAnalysis.jl for convexity certificates »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Friday 1 August 2025 17:22">Friday 1 August 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
